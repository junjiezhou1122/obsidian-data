{"response":"=== Contents ===\n\n\n=== Preface ===\n\n\n=== Biases: An Introduction ===\n\n\n===   Book I: Map and Territory  ===\n\n\n=== Predictably Wrong ===\n\n\n=== Fake Beliefs ===\n\n\n=== Noticing Confusion ===\n\n\n=== Mysterious Answers ===\n\n\n===   Book II: How to Actually Change Your Mind  ===\n\n\n=== Overly Convenient Excuses ===\n\n\n=== Politics and Rationality ===\n\n\n=== Against Rationalization ===\n\n\n=== Against Doublethink ===\n\n\n=== Seeing with Fresh Eyes ===\n\n\n=== Death Spirals ===\n\n\n=== Letting Go ===\n\n\n===   Book III: The Machine in the Ghost  ===\n\n\n=== The Simple Math of Evolution ===\n\n\n=== Fragile Purposes ===\n\n\n=== A Human's Guide to Words ===\n\n\n===   Book IV: Mere Reality  ===\n\n\n=== Lawful Truth ===\n\n\n=== Reductionism 101 ===\n\n\n=== Joy in the Merely Real ===\n\n\n=== Physicalism 201 ===\n\n\n=== Quantum Physics and Many Worlds ===\n\n\n=== Science and Rationality ===\n\n\n===   Book V: Mere Goodness  ===\n\n\n=== Fake Preferences ===\n\n\n=== Value Theory ===\n\n\n=== Quantified Humanism ===\n\n\n===   Book VI: Becoming Stronger  ===\n\n\n=== Yudkowsky's Coming of Age ===\n\n\n=== Challenging the Difficult ===\n\n\n=== The Craft and the Community ===\n\n\n=== Bibliography ===\n--- Page 3 ---\nRationality\n\n--- Page 5 ---\nRationality From AI to Zombies Eliezer Yudkowsky MIRI\n\n--- Page 6 ---\nEliezer Yudkowsky is a Research Fellow at the Machine Intelligence Research Institute. Written by Eliezer Yudkowsky Published in 2015 by the Machine Intelligence Research Institute Berkeley 94704 United States of America intelligence.org Released under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported license. cc © BY-NC-SA 3.0 isbn-10: 1939311144 isbn-13: 978-1-939311-14-6 (pdf) The Machine Intelligence Research Institute gratefully acknowledges the generous support of all those involved in the publication of this book, and the donors who support MIRI’s work in general. iv\n\n--- Page 7 ---\nContents Contents v Preface xviii Biases: An Introduction xxiii Book I Map and Territory A Predictably Wrong 1 What Do I Mean By “Rationality”? 7 2 Feeling Rational 12 3 Why Truth? And . . . 15 4 . . . What’s a Bias, Again? 19 5 Availability 23 6 Burdensome Details 26 7 Planning Fallacy 30 8 Illusion of Transparency: Why No One Understands You 34 9 Expecting Short Inferential Distances 37 10 The Lens That Sees Its Own Flaws 40 v\n\n--- Page 8 ---\nContents B Fake Beliefs 11 Making Beliefs Pay Rent (in Anticipated Experiences) 45 12 A Fable of Science and Politics 49 13 Belief in Belief 54 14 Bayesian Judo 59 15 Pretending to be Wise 61 16 Religion’s Claim to be Non-Disprovable 65 17 Professing and Cheering 69 18 Belief as Attire 72 19 Applause Lights 74 C Noticing Confusion 20 Focus Your Uncertainty 81 21 What Is Evidence? 84 22 Scientific Evidence, Legal Evidence, Rational Evidence 88 23 How Much Evidence Does It Take? 91 24 Einstein’s Arrogance 95 25 Occam’s Razor 98 26 Your Strength as a Rationalist 103 27 Absence of Evidence Is Evidence of Absence 106 28 Conservation of Expected Evidence 109 29 Hindsight Devalues Science 112 D Mysterious Answers 30 Fake Explanations 117 31 Guessing the Teacher’s Password 120 32 Science as Attire 124 33 Fake Causality 127 34 Semantic Stopsigns 132 35 Mysterious Answers to Mysterious Questions 136 36 The Futility of Emergence 140 37 Say Not “Complexity” 143 38 Positive Bias: Look into the Dark 147 39 Lawful Uncertainty 150 40 My Wild and Reckless Youth 154 41 Failing to Learn from History 158 42 Making History Available 160 43 Explain/Worship/Ignore? 163 vi\n\n--- Page 9 ---\n44 “Science” as Curiosity-Stopper 165 45 Truly Part of You 168 Interlude: The Simple Truth 173 Book II How to Actually Change Your Mind Rationality: An Introduction 199 E Overly Convenient Excuses 46 The Proper Use of Humility 211 47 The Third Alternative 216 48 Lotteries: A Waste of Hope 219 49 New Improved Lottery 221 50 But There’s Still a Chance, Right? 224 51 The Fallacy of Gray 227 52 Absolute Authority 232 53 How to Convince Me That 2 + 2 = 3 239 54 Infinite Certainty 242 55 0 And 1 Are Not Probabilities 246 56 Your Rationality Is My Business 250 F Politics and Rationality 57 Politics is the Mind-Killer 255 58 Policy Debates Should Not Appear One-Sided 257 59 The Scales of Justice, the Notebook of Rationality 261 60 Correspondence Bias 264 61 Are Your Enemies Innately Evil? 267 62 Reversed Stupidity Is Not Intelligence 270 63 Argument Screens Off Authority 274 64 Hug the Query 280 65 Rationality and the English Language 282 66 Human Evil and Muddled Thinking 286 vii\n\n--- Page 10 ---\nContents G Against Rationalization 67 Knowing About Biases Can Hurt People 293 68 Update Yourself Incrementally 296 69 One Argument Against An Army 299 70 The Bottom Line 302 71 What Evidence Filtered Evidence? 305 72 Rationalization 309 73 A Rational Argument 312 74 Avoiding Your Belief ’s Real Weak Points 315 75 Motivated Stopping and Motivated Continuation 320 76 Fake Justification 323 77 Is That Your True Rejection? 326 78 Entangled Truths, Contagious Lies 330 79 Of Lies and Black Swan Blowups 334 80 Dark Side Epistemology 335 H Against Doublethink 81 Singlethink 343 82 Doublethink (Choosing to be Biased) 345 83 No, Really, I’ve Deceived Myself 349 84 Belief in Self-Deception 351 85 Moore’s Paradox 356 86 Don’t Believe You’ll Self-Deceive 359 I Seeing with Fresh Eyes 87 Anchoring and Adjustment 365 88 Priming and Contamination 368 89 Do We Believe Everything We’re Told? 371 90 Cached Thoughts 374 91 The “Outside the Box” Box 377 92 Original Seeing 380 93 Stranger than History 383 94 The Logical Fallacy of Generalization from Fictional Evidence 385 95 The Virtue of Narrowness 391 96 How to Seem (and Be) Deep 395 97 We Change Our Minds Less Often Than We Think 399 98 Hold Off On Proposing Solutions 401 99 The Genetic Fallacy 404 viii\n\n--- Page 11 ---\nJ Death Spirals 100 The Affect Heuristic 409 101 Evaluability (and Cheap Holiday Shopping) 413 102 Unbounded Scales, Huge Jury Awards, and Futurism 418 103 The Halo Effect 422 104 Superhero Bias 426 105 Mere Messiahs 430 106 Affective Death Spirals 434 107 Resist the Happy Death Spiral 437 108 Uncritical Supercriticality 443 109 Evaporative Cooling of Group Beliefs 447 110 When None Dare Urge Restraint 451 111 The Robbers Cave Experiment 454 112 Every Cause Wants to Be a Cult 458 113 Guardians of the Truth 461 114 Guardians of the Gene Pool 466 115 Guardians of Ayn Rand 468 116 Two Cult Koans 473 117 Asch’s Conformity Experiment 476 118 On Expressing Your Concerns 480 119 Lonely Dissent 483 120 Cultish Countercultishness 487 K Letting Go 121 The Importance of Saying “Oops” 497 122 The Crackpot Offer 500 123 Just Lose Hope Already 503 124 The Proper Use of Doubt 505 125 You Can Face Reality 508 126 The Meditation on Curiosity 509 127 No One Can Exempt You From Rationality’s Laws 513 128 Leave a Line of Retreat 516 129 Crisis of Faith 520 130 The Ritual 528 ix\n\n--- Page 12 ---\nContents Book III The Machine in the Ghost Minds: An Introduction 539 Interlude: The Power of Intelligence 547 L The Simple Math of Evolution 131 An Alien God 553 132 The Wonder of Evolution 561 133 Evolutions Are Stupid (But Work Anyway) 565 134 No Evolutions for Corporations or Nanodevices 570 135 Evolving to Extinction 576 136 The Tragedy of Group Selectionism 582 137 Fake Optimization Criteria 588 138 Adaptation-Executers, Not Fitness-Maximizers 591 139 Evolutionary Psychology 595 140 An Especially Elegant Evolutionary Psychology Experiment 600 141 Superstimuli and the Collapse of Western Civilization 604 142 Thou Art Godshatter 608 M Fragile Purposes 143 Belief in Intelligence 617 144 Humans in Funny Suits 621 145 Optimization and the Intelligence Explosion 627 146 Ghosts in the Machine 634 147 Artificial Addition 639 148 Terminal Values and Instrumental Values 645 149 Leaky Generalizations 654 150 The Hidden Complexity of Wishes 658 151 Anthropomorphic Optimism 665 152 Lost Purposes 669 N A Human’s Guide to Words 153 The Parable of the Dagger 677 x\n\n--- Page 13 ---\n154 The Parable of Hemlock 679 155 Words as Hidden Inferences 683 156 Extensions and Intensions 686 157 Similarity Clusters 690 158 Typicality and Asymmetrical Similarity 692 159 The Cluster Structure of Thingspace 695 160 Disguised Queries 699 161 Neural Categories 704 162 How An Algorithm Feels From Inside 710 163 Disputing Definitions 716 164 Feel the Meaning 721 165 The Argument from Common Usage 725 166 Empty Labels 730 167 Taboo Your Words 733 168 Replace the Symbol with the Substance 737 169 Fallacies of Compression 742 170 Categorizing Has Consequences 746 171 Sneaking in Connotations 749 172 Arguing “By Definition” 753 173 Where to Draw the Boundary? 757 174 Entropy, and Short Codes 761 175 Mutual Information, and Density in Thingspace 765 176 Superexponential Conceptspace, and Simple Words 772 177 Conditional Independence, and Naive Bayes 779 178 Words as Mental Paintbrush Handles 787 179 Variable Question Fallacies 790 180 37 Ways That Words Can Be Wrong 793 Interlude: An Intuitive Explanation of Bayes’s Theorem 803 Book IV Mere Reality The World: An Introduction 834 xi\n\n--- Page 14 ---\nContents O Lawful Truth 181 Universal Fire 843 182 Universal Law 847 183 Is Reality Ugly? 850 184 Beautiful Probability 855 185 Outside the Laboratory 861 186 The Second Law of Thermodynamics, and Engines of Cognition 867 187 Perpetual Motion Beliefs 875 188 Searching for Bayes-Structure 879 P Reductionism 101 189 Dissolving the Question 887 190 Wrong Questions 892 191 Righting a Wrong Question 895 192 Mind Projection Fallacy 899 193 Probability is in the Mind 901 194 The Quotation is Not the Referent 906 195 Qualitatively Confused 909 196 Think Like Reality 913 197 Chaotic Inversion 916 198 Reductionism 918 199 Explaining vs. Explaining Away 923 200 Fake Reductionism 928 201 Savannah Poets 931 Q Joy in the Merely Real 202 Joy in the Merely Real 939 203 Joy in Discovery 942 204 Bind Yourself to Reality 946 205 If You Demand Magic, Magic Won’t Help 949 206 Mundane Magic 953 207 The Beauty of Settled Science 957 208 Amazing Breakthrough Day: April 1st 959 209 Is Humanism a Religion Substitute? 962 210 Scarcity 965 211 The Sacred Mundane 968 212 To Spread Science, Keep It Secret 972 213 Initiation Ceremony 976 xii\n\n--- Page 15 ---\nR Physicalism 201 214 Hand vs. Fingers 983 215 Angry Atoms 986 216 Heat vs. Motion 991 217 Brain Breakthrough! It’s Made of Neurons! 996 218 When Anthropomorphism Became Stupid 998 219 A Priori 1002 220 Reductive Reference 1006 221 Zombies! Zombies? 1012 222 Zombie Responses 1032 223 The Generalized Anti-Zombie Principle 1040 224 GAZP vs. GLUT 1050 225 Belief in the Implied Invisible 1058 226 Zombies: The Movie 1063 227 Excluding the Supernatural 1068 228 Psychic Powers 1075 S Quantum Physics and Many Worlds 229 Quantum Explanations 1081 230 Configurations and Amplitude 1087 231 Joint Configurations 1098 232 Distinct Configurations 1103 233 Collapse Postulates 1111 234 Decoherence is Simple 1115 235 Decoherence is Falsifiable and Testable 1125 236 Privileging the Hypothesis 1134 237 Living in Many Worlds 1139 238 Quantum Non-Realism 1145 239 If Many-Worlds Had Come First 1154 240 Where Philosophy Meets Science 1163 241 Thou Art Physics 1168 242 Many Worlds, One Best Guess 1173 T Science and Rationality 243 The Failures of Eld Science 1187 244 The Dilemma: Science or Bayes? 1196 245 Science Doesn’t Trust Your Rationality 1201 246 When Science Can’t Help 1205 xiii\n\n--- Page 16 ---\nContents 247 Science Isn’t Strict Enough 1210 248 Do Scientists Already Know This Stuff? 1215 249 No Safe Defense, Not Even Science 1221 250 Changing the Definition of Science 1227 251 Faster Than Science 1229 252 Einstein’s Speed 1233 253 That Alien Message 1241 254 My Childhood Role Model 1250 255 Einstein’s Superpowers 1256 256 Class Project 1261 Interlude: A Technical Explanation of Technical Explanation 1267 Book V Mere Goodness Ends: An Introduction 1321 U Fake Preferences 257 Not for the Sake of Happiness (Alone) 1329 258 Fake Selfishness 1333 259 Fake Morality 1335 260 Fake Utility Functions 1338 261 Detached Lever Fallacy 1342 262 Dreams of AI Design 1348 263 The Design Space of Minds-in-General 1353 V Value Theory 264 Where Recursive Justification Hits Bottom 1359 265 My Kind of Reflection 1368 266 No Universally Compelling Arguments 1372 267 Created Already in Motion 1377 268 Sorting Pebbles into Correct Heaps 1380 269 2-Place and 1-Place Words 1384 270 What Would You Do Without Morality? 1389 xiv\n\n--- Page 17 ---\n271 Changing Your Metaethics 1391 272 Could Anything Be Right? 1395 273 Morality as Fixed Computation 1400 274 Magical Categories 1405 275 The True Prisoner’s Dilemma 1414 276 Sympathetic Minds 1419 277 High Challenge 1424 278 Serious Stories 1428 279 Value is Fragile 1437 280 The Gift We Give to Tomorrow 1443 W Quantified Humanism 281 Scope Insensitivity 1453 282 One Life Against the World 1456 283 The Allais Paradox 1459 284 Zut Allais! 1462 285 Feeling Moral 1469 286 The “Intuitions” Behind “Utilitarianism” 1472 287 Ends Don’t Justify Means (Among Humans) 1480 288 Ethical Injunctions 1485 289 Something to Protect 1493 290 When (Not) to Use Probabilities 1499 291 Newcomb’s Problem and Regret of Rationality 1505 Interlude: The Twelve Virtues of Rationality 1516 Book VI Becoming Stronger Beginnings: An Introduction 1527 X Yudkowsky’s Coming of Age 292 My Childhood Death Spiral 1535 293 My Best and Worst Mistake 1539 294 Raised in Technophilia 1544 xv\n\n--- Page 18 ---\nContents 295 A Prodigy of Refutation 1550 296 The Sheer Folly of Callow Youth 1554 297 That Tiny Note of Discord 1561 298 Fighting a Rearguard Action Against the Truth 1567 299 My Naturalistic Awakening 1571 300 The Level Above Mine 1576 301 The Magnitude of His Own Folly 1580 302 Beyond the Reach of God 1586 303 My Bayesian Enlightenment 1595 Y Challenging the Difficult 304 Tsuyoku Naritai ! (I Want to Become Stronger) 1605 305 Tsuyoku vs. the Egalitarian Instinct 1608 306 Trying to Try 1610 307 Use the Try Harder, Luke 1614 308 On Doing the Impossible 1617 309 Make an Extraordinary Effort 1624 310 Shut Up and Do the Impossible! 1629 311 Final Words 1639 Z The Craft and the Community 312 Raising the Sanity Waterline 1651 313 A Sense That More Is Possible 1655 314 Epistemic Viciousness 1660 315 Schools Proliferating Without Evidence 1663 316 Three Levels of Rationality Verification 1666 317 Why Our Kind Can’t Cooperate 1670 318 Tolerate Tolerance 1678 319 Your Price for Joining 1681 320 Can Humanism Match Religion’s Output? 1686 321 Church vs. Taskforce 1691 322 Rationality: Common Interest of Many Causes 1696 323 Helpless Individuals 1700 324 Money: The Unit of Caring 1703 325 Purchase Fuzzies and Utilons Separately 1707 326 Bystander Apathy 1712 327 Collective Apathy and the Internet 1716 328 Incremental Progress and the Valley 1719 xvi\n\n--- Page 19 ---\n329 Bayesians vs. Barbarians 1724 330 Beware of Other-Optimizing 1731 331 Practical Advice Backed by Deep Theories 1736 332 The Sin of Underconfidence 1740 333 Go Forth and Create the Art! 1746 Bibliography xvii\n\n--- Page 20 ---\nPreface You hold in your hands a compilation of two years of daily blog posts. In retrospect, I look back on that project and see a large number of things I did completely wrong. I’m fine with that. Looking back and not seeing a huge number of things I did wrong would mean that neither my writing nor my understanding had improved since 2009. Oops is the sound we make when we improve our beliefs and strategies; so to look back at a time and not see anything you did wrong means that you haven’t learned anything or changed your mind since then. It was a mistake that I didn’t write my two years of blog posts with the intention of helping people do better in their everyday lives. I wrote it with the intention of helping people solve big, difficult, important problems, and I chose impressive-sounding, abstract problems as my examples. In retrospect, this was the second-largest mistake in my approach. It ties in to the first -largest mistake in my writing, which was that I didn’t realize that the big problem in learning this valuable way of thinking was figuring out how to practice it, not knowing the theory. I didn’t realize that part was the priority; and regarding this I can only say “Oops” and “Duh.” Yes, sometimes those big issues really are big and really are important; but that doesn’t change the basic truth that to master skills you need to practice them and it’s harder to practice on things that are further away. (Today the xviii\n\n--- Page 21 ---\nCenter for Applied Rationality is working on repairing this huge mistake of mine in a more systematic fashion.) A third huge mistake I made was to focus too much on rational belief, too little on rational action. The fourth-largest mistake I made was that I should have better organized the content I was presenting in the sequences. In particular, I should have created a wiki much earlier, and made it easier to read the posts in sequence. That mistake at least is correctable. In the present work Rob Bensinger has reordered the posts and reorganized them as much as he can without trying to rewrite all the actual material (though he’s rewritten a bit of it). My fifth huge mistake was that I—as I saw it—tried to speak plainly about the stupidity of what appeared to me to be stupid ideas. I did try to avoid the fallacy known as Bulverism, which is where you open your discussion by talking about how stupid people are for believing something; I would always discuss the issue first, and only afterwards say, “And so this is stupid.” But in 2009 it was an open question in my mind whether it might be important to have some people around who expressed contempt for homeopathy. I thought, and still do think, that there is an unfortunate problem wherein treating ideas courteously is processed by many people on some level as “Nothing bad will happen to me if I say I believe this; I won’t lose status if I say I believe in homeopathy,” and that derisive laughter by comedians can help people wake up from the dream. Today I would write more courteously, I think. The discourtesy did serve a function, and I think there were people who were helped by reading it; but I now take more seriously the risk of building communities where the normal and expected reaction to low-status outsider views is open mockery and contempt. Despite my mistake, I am happy to say that my readership has so far been amazingly good about not using my rhetoric as an excuse to bully or belittle others. (I want to single out Scott Alexander in particular here, who is a nicer person than I am and an increasingly amazing writer on these topics, and may deserve part of the credit for making the culture of Less Wrong a healthy one.) To be able to look backwards and say that you’ve “failed” implies that you had goals. So what was it that I was trying to do? xix\n\n--- Page 22 ---\nPreface There is a certain valuable way of thinking, which is not yet taught in schools, in this present day. This certain way of thinking is not taught systematically at all. It is just absorbed by people who grow up reading books like Surely You’re Joking, Mr. Feynman or who have an unusually great teacher in high school. Most famously, this certain way of thinking has to do with science, and with the experimental method. The part of science where you go out and look at the universe instead of just making things up. The part where you say “Oops” and give up on a bad theory when the experiments don’t support it. But this certain way of thinking extends beyond that. It is deeper and more universal than a pair of goggles you put on when you enter a laboratory and take off when you leave. It applies to daily life, though this part is subtler and more difficult. But if you can’t say “Oops” and give up when it looks like something isn’t working, you have no choice but to keep shooting yourself in the foot. You have to keep reloading the shotgun and you have to keep pulling the trigger. You know people like this. And somewhere, someplace in your life you’d rather not think about, you are people like this. It would be nice if there was a certain way of thinking that could help us stop doing that. In spite of how large my mistakes were, those two years of blog posting appeared to help a surprising number of people a surprising amount. It didn’t work reliably, but it worked sometimes. In modern society so little is taught of the skills of rational belief and decision-making, so little of the mathematics and sciences underlying them . . . that it turns out that just reading through a massive brain-dump full of problems in philosophy and science can, yes, be surprisingly good for you. Walking through all of that, from a dozen different angles, can sometimes convey a glimpse of the central rhythm. Because it is all, in the end, one thing. I talked about big important distant problems and neglected immediate life, but the laws governing them aren’t actually different. There are huge gaps in which parts I focused on, and I picked all the wrong examples; but it is all in the end one thing. I am proud to look back and say that, even after all the mistakes I made, and all the other times I said “Oops” . . . xx\n\n--- Page 23 ---\nEven five years later, it still appears to me that this is better than nothing. —Eliezer Yudkowsky, February 2015 xxi\n\n--- Page 25 ---\nBiases: An Introduction by Rob Bensinger It’s not a secret. For some reason, though, it rarely comes up in conversation, and few people are asking what we should do about it. It’s a pattern, hidden unseen behind all our triumphs and failures, unseen behind our eyes. What is it? Imagine reaching into an urn that contains seventy white balls and thirty red ones, and plucking out ten mystery balls. Perhaps three of the ten balls will be red, and you’ll correctly guess how many red balls total were in the urn. Or perhaps you’ll happen to grab four red balls, or some other number. Then you’ll probably get the total number wrong. This random error is the cost of incomplete knowledge, and as errors go, it’s not so bad. Your estimates won’t be incorrect on average , and the more you learn, the smaller your error will tend to be. On the other hand, suppose that the white balls are heavier, and sink to the bottom of the urn. Then your sample may be unrepresentative in a consistent direction . That sort of error is called “statistical bias.” When your method of learning about the world is biased, learning more may not help. Acquiring more data can even consistently worsen a biased prediction. xxiii\n\n--- Page 26 ---\nBiases: An Introduction If you’re used to holding knowledge and inquiry in high esteem, this is a scary prospect. If we want to be sure that learning more will help us, rather than making us worse off than we were before, we need to discover and correct for biases in our data. The idea of cognitive bias in psychology works in an analogous way. A cognitive bias is a systematic error in how we think , as opposed to a random error or one that’s merely caused by our ignorance. Whereas statistical bias skews a sample so that it less closely resembles a larger population, cognitive biases skew our beliefs so that they less accurately represent the facts, and they skew our decision-making so that it less reliably achieves our goals. Maybe you have an optimism bias, and you find out that the red balls can be used to treat a rare tropical disease besetting your brother. You may then overestimate how many red balls the urn contains because you wish the balls were mostly red. Here, your sample isn’t what’s biased. You’re what’s biased. Now that we’re talking about biased people , however, we have to be careful. Usually, when we call individuals or groups “biased,” we do it to chastise them for being unfair or partial. Cognitive bias is a different beast altogether. Cognitive biases are a basic part of how humans in general think, not the sort of defect we could blame on a terrible upbringing or a rotten personality. 1 A cognitive bias is a systematic way that your innate patterns of thought fall short of truth (or some other attainable goal, such as happiness). Like statistical biases, cognitive biases can distort our view of reality, they can’t always be fixed by just gathering more data, and their effects can add up over time. But when the miscalibrated measuring instrument you’re trying to fix is you , debiasing is a unique challenge. Still, this is an obvious place to start. For if you can’t trust your brain, how can you trust anything else? It would be useful to have a name for this project of overcoming cognitive bias, and of overcoming all species of error where our minds can come to undermine themselves. We could call this project whatever we’d like. For the moment, though, I suppose “rationality” is as good a name as any. xxiv\n\n--- Page 27 ---\nRational Feelings In a Hollywood movie, being “rational” usually means that you’re a stern, hyperintellectual stoic. Think Spock from Star Trek , who “rationally” sup- presses his emotions, “rationally” refuses to rely on intuitions or impulses, and is easily dumbfounded and outmaneuvered upon encountering an erratic or “irrational” opponent. 2 There’s a completely different notion of “rationality” studied by mathemati- cians, psychologists, and social scientists. Roughly, it’s the idea of doing the best you can with what you’ve got . A rational person, no matter how out of their depth they are, forms the best beliefs they can with the evidence they’ve got. A rational person, no matter how terrible a situation they’re stuck in, makes the best choices they can to improve their odds of success. Real-world rationality isn’t about ignoring your emotions and intuitions. For a human, rationality often means becoming more self-aware about your feelings, so you can factor them into your decisions. Rationality can even be about knowing when not to overthink things. When selecting a poster to put on their wall, or predicting the outcome of a basket- ball game, experimental subjects have been found to perform worse if they carefully analyzed their reasons. 3,4 There are some problems where conscious deliberation serves us better, and others where snap judgments serve us better. Psychologists who work on dual process theories distinguish the brain’s “System 1” processes (fast, implicit, associative, automatic cognition) from its “System 2” processes (slow, explicit, intellectual, controlled cognition). 5 The stereotype is for rationalists to rely entirely on System 2, disregarding their feelings and impulses. Looking past the stereotype, someone who is actually being rational—actually achieving their goals, actually mitigating the harm from their cognitive biases—would rely heavily on System-1 habits and intuitions where they’re reliable. Unfortunately, System 1 on its own seems to be a terrible guide to “when should I trust System 1?” Our untrained intuitions don’t tell us when we ought to stop relying on them. Being biased and being unbiased feel the same. 6 xxv\n\n--- Page 28 ---\nBiases: An Introduction On the other hand, as behavioral economist Dan Ariely notes: we’re pre- dictably irrational. We screw up in the same ways, again and again, systemati- cally. If we can’t use our gut to figure out when we’re succumbing to a cognitive bias, we may still be able to use the sciences of mind. The Many Faces of Bias To solve problems, our brains have evolved to employ cognitive heuristics— rough shortcuts that get the right answer often, but not all the time. Cognitive biases arise when the corners cut by these heuristics result in a relatively con- sistent and discrete mistake. The representativeness heuristic, for example, is our tendency to assess phenomena by how representative they seem of various categories. This can lead to biases like the conjunction fallacy . Tversky and Kahneman found that experimental subjects considered it less likely that a strong tennis player would “lose the first set” than that he would “lose the first set but win the match.” 7 Making a comeback seems more typical of a strong player, so we overestimate the probability of this complicated-but-sensible-sounding narrative compared to the probability of a strictly simpler scenario. The representativeness heuristic can also contribute to base rate neglect , where we ground our judgments in how intuitively “normal” a combination of attributes is, neglecting how common each attribute is in the population at large. 8 Is it more likely that Steve is a shy librarian, or that he’s a shy salesper- son? Most people answer this kind of question by thinking about whether “shy” matches their stereotypes of those professions. They fail to take into considera- tion how much more common salespeople are than librarians—seventy-five times as common, in the United States. 9 Other examples of biases include duration neglect (evaluating experiences without regard to how long they lasted), the sunk cost fallacy (feeling committed to things you’ve spent resources on in the past, when you should be cutting your losses and moving on), and confirmation bias (giving more weight to evidence that confirms what we already believe). 10,11 xxvi\n\n--- Page 29 ---\nKnowing about a bias, however, is rarely enough to protect you from it. In a study of bias blindness , experimental subjects predicted that if they learned a painting was the work of a famous artist, they’d have a harder time neutrally assessing the quality of the painting. And, indeed, subjects who were told a painting’s author and were asked to evaluate its quality exhibited the very bias they had predicted, relative to a control group. When asked afterward , however, the very same subjects claimed that their assessments of the paintings had been objective and unaffected by the bias—in all groups! 12,13 We’re especially loathe to think of our views as inaccurate compared to the views of others. Even when we correctly identify others’ biases, we have a special bias blind spot when it comes to our own flaws. 14 We fail to detect any “biased-feeling thoughts” when we introspect, and so draw the conclusion that we must just be more objective than everyone else. 15 Studying biases can in fact make you more vulnerable to overconfidence and confirmation bias, as you come to see the influence of cognitive biases all around you—in everyone but yourself. And the bias blind spot, unlike many biases, is especially severe among people who are especially intelligent, thoughtful, and open-minded . 16,17 This is cause for concern. Still . . . it does seem like we should be able to do better. It’s known that we can reduce base rate neglect by thinking of probabilities as frequencies of objects or events. We can minimize duration neglect by directing more attention to duration and depicting it graphically. 18 People vary in how strongly they exhibit different biases, so there should be a host of yet-unknown ways to influence how biased we are. If we want to improve, however, it’s not enough for us to pore over lists of cognitive biases. The approach to debiasing in Rationality: From AI to Zombies is to communicate a systematic understanding of why good reasoning works, and of how the brain falls short of it. To the extent this volume does its job, its approach can be compared to the one described in Serfas, who notes that “years of financially related work experience” didn’t affect people’s susceptibility to the sunk cost bias, whereas “the number of accounting courses attended” did help. xxvii\n\n--- Page 30 ---\nBiases: An Introduction As a consequence, it might be necessary to distinguish between experience and expertise, with expertise meaning “the develop- ment of a schematic principle that involves conceptual under- standing of the problem,” which in turn enables the decision maker to recognize particular biases. However, using expertise as countermeasure requires more than just being familiar with the situational content or being an expert in a particular domain. It requires that one fully understand the underlying rationale of the respective bias, is able to spot it in the particular setting, and also has the appropriate tools at hand to counteract the bias. 19 The goal of this book is to lay the groundwork for creating rationality “exper- tise.” That means acquiring a deep understanding of the structure of a very general problem: human bias, self-deception, and the thousand paths by which sophisticated thought can defeat itself. A Word About This Text Rationality: From AI to Zombies began its life as a series of essays by Eliezer Yudkowsky, published between 2006 and 2009 on the economics blog Over- coming Bias and its spin-off community blog Less Wrong . I’ve worked with Yudkowsky for the last year at the Machine Intelligence Research Institute (MIRI), a nonprofit he founded in 2000 to study the theoretical requirements for smarter-than-human artificial intelligence (AI). Reading his blog posts got me interested in his work. He impressed me with his ability to concisely communicate insights it had taken me years of studying analytic philosophy to internalize. In seeking to reconcile science’s anarchic and skeptical spirit with a rigorous and systematic approach to inquiry, Yudkowsky tries not just to refute but to understand the many false steps and blind alleys bad philosophy (and bad lack-of-philosophy) can produce. My hope in helping organize these essays into a book is to make it easier to dive in to them, and easier to appreciate them as a coherent whole. xxviii\n\n--- Page 31 ---\nThe resultant rationality primer is frequently personal and irreverent— drawing, for example, from Yudkowsky’s experiences with his Orthodox Jew- ish mother (a psychiatrist) and father (a physicist), and from conversations on chat rooms and mailing lists. Readers who are familiar with Yudkowsky from Harry Potter and the Methods of Rationality , his science-oriented take- off of J.K. Rowling’s Harry Potter books, will recognize the same irreverent iconoclasm, and many of the same core concepts. Stylistically, the essays in this book run the gamut from “lively textbook” to “compendium of thoughtful vignettes” to “riotous manifesto,” and the content is correspondingly varied. Rationality: From AI to Zombies collects hundreds of Yudkowsky’s blog posts into twenty-six “sequences,” chapter-like series of thematically linked posts. The sequences in turn are grouped into six books, covering the following topics: Book 1— Map and Territory . What is a belief, and what makes some beliefs work better than others? These four sequences explain the Bayesian notions of rationality, belief, and evidence. A running theme: the things we call “explanations” or “theories” may not always function like maps for navigating the world. As a result, we risk mixing up our mental maps with the other objects in our toolbox. Book 2— How to Actually Change Your Mind . This truth thing seems pretty handy. Why, then, do we keep jumping to conclusions, digging our heels in, and recapitulating the same mistakes? Why are we so bad at acquiring accurate beliefs, and how can we do better? These seven sequences discuss mo- tivated reasoning and confirmation bias, with a special focus on hard-to-spot species of self-deception and the trap of “using arguments as soldiers.” Book 3— The Machine in the Ghost . Why haven’t we evolved to be more rational? Even taking into account resource constraints, it seems like we could be getting a lot more epistemic bang for our evidential buck. To get a realistic picture of how and why our minds execute their biological functions, we need to crack open the hood and see how evolution works, and how our brains work, with more precision. These three sequences illustrate how even philosophers and scientists can be led astray when they rely on intuitive, non-technical evolutionary or psychological accounts. By locating our minds within a larger xxix\n\n--- Page 32 ---\nBiases: An Introduction space of goal-directed systems, we can identify some of the peculiarities of human reasoning and appreciate how such systems can “lose their purpose.” Book 4— Mere Reality . What kind of world do we live in? What is our place in that world? Building on the previous sequences’ examples of how evo- lutionary and cognitive models work, these six sequences explore the nature of mind and the character of physical law. In addition to applying and general- izing past lessons on scientific mysteries and parsimony, these essays raise new questions about the role science should play in individual rationality. Book 5— Mere Goodness . What makes something valuable —morally, or aesthetically, or prudentially? These three sequences ask how we can justify, revise, and naturalize our values and desires. The aim will be to find a way to understand our goals without compromising our efforts to actually achieve them. Here the biggest challenge is knowing when to trust your messy, com- plicated case-by-case impulses about what’s right and wrong, and when to replace them with simple exceptionless principles. Book 6— Becoming Stronger . How can individuals and communities put all this into practice? These three sequences begin with an autobiographical account of Yudkowsky’s own biggest philosophical blunders, with advice on how he thinks others might do better. The book closes with recommendations for developing evidence-based applied rationality curricula, and for forming groups and institutions to support interested students, educators, researchers, and friends. The sequences are also supplemented with “interludes,” essays taken from Yudkowsky’s personal website, http://www.yudkowsky.net . These tie in to the sequences in various ways; e.g., The Twelve Virtues of Rationality poetically summarizes many of the lessons of Rationality: From AI to Zombies , and is often quoted in other essays. Clicking the asterisk at the bottom of an essay will take you to the original version of it on Less Wrong (where you can leave comments) or on Yudkowsky’s website. You can also find a glossary for Rationality: From AI to Zombies terminology online, at http://wiki.lesswrong.com/wiki/RAZ_Glossary. xxx\n\n--- Page 33 ---\nMap and Territory This, the first book, begins with a sequence on cognitive bias: “ Predictably Wrong .” The rest of the book won’t stick to just this topic; bad habits and bad ideas matter, even when they arise from our minds’ contents as opposed to our minds’ structure. Thus evolved and invented errors will both be on display in subsequent sequences, beginning with a discussion in “ Fake Beliefs ” of ways that one’s expectations can come apart from one’s professed beliefs. An account of irrationality would also be incomplete if it provided no theory about how rationality works—or if its “theory” only consisted of vague truisms, with no precise explanatory mechanism. The “ Noticing Confusion ” sequence asks why it’s useful to base one’s behavior on “rational” expectations, and what it feels like to do so. “ Mysterious Answers ” next asks whether science resolves these problems for us. Scientists base their models on repeatable experiments, not speculation or hearsay. And science has an excellent track record compared to anecdote, religion, and . . . pretty much everything else. Do we still need to worry about “fake” beliefs, confirmation bias, hindsight bias, and the like when we’re working with a community of people who want to explain phenomena, not just tell appealing stories? This is then followed by The Simple Truth , a stand-alone allegory on the nature of knowledge and belief. It is cognitive bias, however, that provides the clearest and most direct glimpse into the stuff of our psychology, into the shape of our heuristics and the logic of our limitations. It is with bias that we will begin. There is a passage in the Zhuangzi , a proto-Daoist philosophical text, that says: “The fish trap exists because of the fish; once you’ve gotten the fish, you can forget the trap.” 20 I invite you to explore this book in that spirit. Use it like you’d use a fish trap, ever mindful of the purpose you have for it. Carry with you what you can use, so long as it continues to have use; discard the rest. And may your purpose serve you well. xxxi\n\n--- Page 34 ---\nBiases: An Introduction Acknowledgments I am stupendously grateful to Nate Soares, Elizabeth Tarleton, Paul Crow- ley, Brienne Strohl, Adam Freese, Helen Toner, and dozens of volunteers for proofreading portions of this book. Special and sincere thanks to Alex Vermeer, who steered this book to com- pletion, and Tsvi Benson-Tilsen, who combed through the entire book to ensure its readability and consistency. 1. The idea of personal bias, media bias, etc. resembles statistical bias in that it’s an error . Other ways of generalizing the idea of “bias” focus instead on its association with nonrandomness. In machine learning, for example, an inductive bias is just the set of assumptions a learner uses to derive predictions from a data set. Here, the learner is “biased” in the sense that it’s pointed in a specific direction; but since that direction might be truth , it isn’t a bad thing for an agent to have an inductive bias. It’s valuable and necessary. This distinguishes inductive “bias” quite clearly from the other kinds of bias. 2. A sad coincidence: Leonard Nimoy, the actor who played Spock, passed away just a few days before the release of this book. Though we cite his character as a classic example of fake “Hollywood rationality,” we mean no disrespect to Nimoy’s memory. 3. Timothy D. Wilson et al., “Introspecting About Reasons Can Reduce Post-choice Satisfaction,” Personality and Social Psychology Bulletin 19 (1993): 331–331. 4. Jamin Brett Halberstadt and Gary M. Levine, “Effects of Reasons Analysis on the Accuracy of Predicting Basketball Games,” Journal of Applied Social Psychology 29, no. 3 (1999): 517–530. 5. Keith E. Stanovich and Richard F. West, “Individual Differences in Reasoning: Implications for the Rationality Debate?,” Behavioral and Brain Sciences 23, no. 5 (2000): 645–665, http://journals. cambridge.org/abstract_S0140525X00003435. 6. Timothy D. Wilson, David B. Centerbar, and Nancy Brekke, “Mental Contamination and the Debiasing Problem,” in Heuristics and Biases: The Psychology of Intuitive Judgment , ed. Thomas Gilovich, Dale Griffin, and Daniel Kahneman (Cambridge University Press, 2002). xxxii\n\n--- Page 35 ---\n7. Amos Tversky and Daniel Kahneman, “Extensional Versus Intuitive Reasoning: The Con- junction Fallacy in Probability Judgment,” Psychological Review 90, no. 4 (1983): 293–315, doi:10.1037/0033-295X.90.4.293. 8. Richards J. Heuer, Psychology of Intelligence Analysis (Center for the Study of Intelligence, Central Intelligence Agency, 1999). 9. Wayne Weiten, Psychology: Themes and Variations, Briefer Version, Eighth Edition (Cengage Learn- ing, 2010). 10. Raymond S. Nickerson, “Confirmation Bias: A Ubiquitous Phenomenon in Many Guises,” Review of General Psychology 2, no. 2 (1998): 175. 11. Probability neglect is another cognitive bias. In the months and years following the September 11 attacks, many people chose to drive long distances rather than fly. Hijacking wasn’t likely , but it now felt like it was on the table; the mere possibility of hijacking hugely impacted decisions. By relying on black-and-white reasoning (cars and planes are either “safe” or “unsafe,” full stop), people actually put themselves in much more danger. Where they should have weighed the probability of dying on a cross-country car trip against the probability of dying on a cross-country flight—the former is hundreds of times more likely—they instead relied on their general feeling of worry and anxiety (the affect heuristic). We can see the same pattern of behavior in children who, hearing arguments for and against the safety of seat belts, hop back and forth between thinking seat belts are a completely good idea or a completely bad one, instead of trying to compare the strengths of the pro and con considerations. 21 Some more examples of biases are: the peak/end rule (evaluating remembered events based on their most intense moment, and how they ended); anchoring (basing decisions on recently encountered information, even when it’s irrelevant) 22 and self-anchoring (using yourself as a model for others’ likely characteristics, without giving enough thought to ways you’re atypical); 23 and status quo bias (excessively favoring what’s normal and expected over what’s new and different). 24 12. Katherine Hansen et al., “People Claim Objectivity After Knowingly Using Biased Strategies,” Personality and Social Psychology Bulletin 40, no. 6 (2014): 691–699. 13. Similarly, Pronin writes of gender bias blindness: In one study, participants considered a male and a female candidate for a police- chief job and then assessed whether being “streetwise” or “formally educated” was more important for the job. The result was that participants favored whichever background they were told the male candidate possessed (e.g., if told he was “streetwise,” they viewed that as more important). Participants were completely blind to this gender bias; indeed, the more objective they believed they had been, the more bias they actually showed. 25 xxxiii\n\n--- Page 36 ---\nBiases: An Introduction Even when we know about biases, Pronin notes, we remain “naive realists” about our own beliefs. We reliably fall back into treating our beliefs as distortion-free representations of how things actually are. 26 14. In a survey of 76 people waiting in airports, individuals rated themselves much less susceptible to cognitive biases on average than a typical person in the airport. In particular, people think of themselves as unusually unbiased when the bias is socially undesirable or has difficult-to-notice consequences. 27 Other studies find that people with personal ties to an issue see those ties as enhancing their insight and objectivity; but when they see other people exhibiting the same ties, they infer that those people are overly attached and biased. 15. Joyce Ehrlinger, Thomas Gilovich, and Lee Ross, “Peering Into the Bias Blind Spot: People’s Assessments of Bias in Themselves and Others,” Personality and Social Psychology Bulletin 31, no. 5 (2005): 680–692. 16. Richard F. West, Russell J. Meserve, and Keith E. Stanovich, “Cognitive Sophistication Does Not Attenuate the Bias Blind Spot,” Journal of Personality and Social Psychology 103, no. 3 (2012): 506. 17. . . . Not to be confused with people who think they’re unusually intelligent, thoughtful, etc. because of the illusory superiority bias. 18. Michael J. Liersch and Craig R. M. McKenzie, “Duration Neglect by Numbers and Its Elimination by Graphs,” Organizational Behavior and Human Decision Processes 108, no. 2 (2009): 303–314. 19. Sebastian Serfas, Cognitive Biases in the Capital Investment Context: Theoretical Considerations and Empirical Experiments on Violations of Normative Rationality (Springer, 2010). 20. Zhuangzi and Burton Watson, The Complete Works of Zhuangzi (Columbia University Press, 1968). 21. Cass R. Sunstein, “Probability Neglect: Emotions, Worst Cases, and Law,” Yale Law Journal (2002): 61–107. 22. Dan Ariely, Predictably Irrational: The Hidden Forces That Shape Our Decisions (HarperCollins, 2008). 23. Boaz Keysar and Dale J. Barr, “Self-Anchoring in Conversation: Why Language Users Do Not Do What They ‘Should,”’ in Heuristics and Biases: The Psychology of Intuitive Judgment: The Psychology of Intuitive Judgment , ed. Griffin Gilovich and Daniel Kahneman (New York: Cambridge University Press, 2002), 150–166, doi:10.2277/0521796792. 24. Scott Eidelman and Christian S. Crandall, “Bias in Favor of the Status Quo,” Social and Personality Psychology Compass 6, no. 3 (2012): 270–281. 25. Eric Luis Uhlmann and Geoffrey L. Cohen, “‘I think it, therefore it’s true’: Effects of Self-perceived Objectivity on Hiring Discrimination,” Organizational Behavior and Human Decision Processes 104, no. 2 (2007): 207–223. xxxiv\n\n--- Page 37 ---\n26. Emily Pronin, “How We See Ourselves and How We See Others,” Science 320 (2008): 1177–1180, http://psych.princeton.edu/psychology/research/pronin/pubs/2008%20Self%20and%20Other.pdf. 27. Emily Pronin, Daniel Y. Lin, and Lee Ross, “The Bias Blind Spot: Perceptions of Bias in Self versus Others,” Personality and Social Psychology Bulletin 28, no. 3 (2002): 369–381. xxxv\n\n--- Page 39 ---\nBook I\n\n--- Page 41 ---\nContents Map and Territory A Predictably Wrong 1 What Do I Mean By “Rationality”? 7 2 Feeling Rational 12 3 Why Truth? And . . . 15 4 . . . What’s a Bias, Again? 19 5 Availability 23 6 Burdensome Details 26 7 Planning Fallacy 30 8 Illusion of Transparency: Why No One Understands You 34 9 Expecting Short Inferential Distances 37 10 The Lens That Sees Its Own Flaws 40 B Fake Beliefs 11 Making Beliefs Pay Rent (in Anticipated Experiences) 45 12 A Fable of Science and Politics 49 13 Belief in Belief 54 14 Bayesian Judo 59 15 Pretending to be Wise 61 16 Religion’s Claim to be Non-Disprovable 65 17 Professing and Cheering 69 18 Belief as Attire 72 3\n\n--- Page 42 ---\nContents 19 Applause Lights 74 C Noticing Confusion 20 Focus Your Uncertainty 81 21 What Is Evidence? 84 22 Scientific Evidence, Legal Evidence, Rational Evidence 88 23 How Much Evidence Does It Take? 91 24 Einstein’s Arrogance 95 25 Occam’s Razor 98 26 Your Strength as a Rationalist 103 27 Absence of Evidence Is Evidence of Absence 106 28 Conservation of Expected Evidence 109 29 Hindsight Devalues Science 112 D Mysterious Answers 30 Fake Explanations 117 31 Guessing the Teacher’s Password 120 32 Science as Attire 124 33 Fake Causality 127 34 Semantic Stopsigns 132 35 Mysterious Answers to Mysterious Questions 136 36 The Futility of Emergence 140 37 Say Not “Complexity” 143 38 Positive Bias: Look into the Dark 147 39 Lawful Uncertainty 150 40 My Wild and Reckless Youth 154 41 Failing to Learn from History 158 42 Making History Available 160 43 Explain/Worship/Ignore? 163 44 “Science” as Curiosity-Stopper 165 45 Truly Part of You 168 Interlude: The Simple Truth 173 4\n\n--- Page 43 ---\nPart A Predictably Wrong\n\n--- Page 45 ---\n1 What Do I Mean By “Rationality”? I mean: 1. Epistemic rationality : systematically improving the accuracy of your beliefs. 2. Instrumental rationality : systematically achieving your values. When you open your eyes and look at the room around you, you’ll locate your laptop in relation to the table, and you’ll locate a bookcase in relation to the wall. If something goes wrong with your eyes, or your brain, then your mental model might say there’s a bookcase where no bookcase exists, and when you go over to get a book, you’ll be disappointed. This is what it’s like to have a false belief, a map of the world that doesn’t correspond to the territory. Epistemic rationality is about building accurate maps instead. This correspondence between belief and reality is commonly called “truth,” and I’m happy to call it that. Instrumental rationality, on the other hand, is about steering reality— sending the future where you want it to go. It’s the art of choosing actions that lead to outcomes ranked higher in your preferences. I sometimes call this “winning.” 7\n\n--- Page 46 ---\nWhat Do I Mean By “Rationality”? So rationality is about forming true beliefs and making winning decisions. Pursuing “truth” here doesn’t mean dismissing uncertain or indirect evi- dence. Looking at the room around you and building a mental map of it isn’t different, in principle, from believing that the Earth has a molten core, or that Julius Caesar was bald. Those questions, being distant from you in space and time, might seem more airy and abstract than questions about your bookcase. Yet there are facts of the matter about the state of the Earth’s core in 2015 CE and about the state of Caesar’s head in 50 BCE. These facts may have real effects upon you even if you never find a way to meet Caesar or the core face-to-face. And “winning” here need not come at the expense of others. The project of life can be about collaboration or self-sacrifice, rather than about competition. “Your values” here means anything you care about , including other people. It isn’t restricted to selfish values or unshared values. When people say “ X is rational!” it’s usually just a more strident way of saying “I think X is true” or “I think X is good.” So why have an additional word for “rational” as well as “true” and “good”? An analogous argument can be given against using “true.” There is no need to say “it is true that snow is white” when you could just say “snow is white.” What makes the idea of truth useful is that it allows us to talk about the general features of map-territory correspondence. “True models usually produce better experimental predictions than false models” is a useful generalization, and it’s not one you can make without using a concept like “true” or “accurate.” Similarly, “Rational agents make decisions that maximize the probabilistic expectation of a coherent utility function” is the kind of thought that depends on a concept of (instrumental) rationality, whereas “It’s rational to eat vegeta- bles” can probably be replaced with “It’s useful to eat vegetables” or “It’s in your interest to eat vegetables.” We need a concept like “rational” in order to note general facts about those ways of thinking that systematically produce truth or value—and the systematic ways in which we fall short of those standards. Sometimes experimental psychologists uncover human reasoning that seems very strange. For example , someone rates the probability “Bill plays jazz” as less than the probability “Bill is an accountant who plays jazz.” This seems like an odd judgment, since any particular jazz-playing accountant is ob- 8\n\n--- Page 47 ---\nPredictably Wrong viously a jazz player. But to what higher vantage point do we appeal in saying that the judgment is wrong ? Experimental psychologists use two gold standards: probability theory , and decision theory . Probability theory is the set of laws underlying rational belief. The math- ematics of probability describes equally and without distinction (a) figuring out where your bookcase is, (b) figuring out the temperature of the Earth’s core, and (c) estimating how many hairs were on Julius Caesar’s head. It’s all the same problem of how to process the evidence and observations to revise (“update”) one’s beliefs. Similarly, decision theory is the set of laws underly- ing rational action, and is equally applicable regardless of what one’s goals and available options are. Let “ P ( such-and-such ) ” stand for “the probability that such-and-such hap- pens,” and P ( A, B ) for “the probability that both A and B happen.” Since it is a universal law of probability theory that P ( A ) ≥ P ( A, B ) , the judgment that P ( Bill plays jazz ) is less than P ( Bill plays jazz, Bill is an accountant ) is labeled incorrect. To keep it technical, you would say that this probability judgment is non- Bayesian . Beliefs and actions that are rational in this mathematically well- defined sense are called “Bayesian.” Note that the modern concept of rationality is not about reasoning in words. I gave the example of opening your eyes, looking around you, and building a mental model of a room containing a bookcase against the wall. The modern concept of rationality is general enough to include your eyes and your brain’s visual areas as things-that-map. It includes your wordless intuitions as well. The math doesn’t care whether we use the same English-language word, “rational,” to refer to Spock and to refer to Bayesianism. The math models good ways of achieving goals or mapping the world, regardless of whether those ways fit our preconceptions and stereotypes about what “rationality” is supposed to be. This does not quite exhaust the problem of what is meant in practice by “rationality,” for two major reasons: First, the Bayesian formalisms in their full form are computationally in- tractable on most real-world problems. No one can actually calculate and obey 9\n\n--- Page 48 ---\nWhat Do I Mean By “Rationality”? the math, any more than you can predict the stock market by calculating the movements of quarks. This is why there is a whole site called “Less Wrong,” rather than a single page that simply states the formal axioms and calls it a day. There’s a whole further art to finding the truth and accomplishing value from inside a human mind : we have to learn our own flaws, overcome our biases, prevent ourselves from self-deceiving, get ourselves into good emotional shape to confront the truth and do what needs doing, et cetera, et cetera. Second, sometimes the meaning of the math itself is called into question. The exact rules of probability theory are called into question by, e.g., anthropic problems in which the number of observers is uncertain. The exact rules of decision theory are called into question by, e.g., Newcomblike problems in which other agents may predict your decision before it happens. 1 In cases like these, it is futile to try to settle the problem by coming up with some new definition of the word “rational” and saying, “Therefore my preferred answer, by definition , is what is meant by the word ‘rational.’ ” This simply raises the question of why anyone should pay attention to your definition. I’m not interested in probability theory because it is the holy word handed down from Laplace. I’m interested in Bayesian-style belief-updating ( with Occam priors ) because I expect that this style of thinking gets us systematically closer to, you know, accuracy , the map that reflects the territory. And then there are questions of how to think that seem not quite answered by either probability theory or decision theory—like the question of how to feel about the truth once you have it . Here, again, trying to define “rationality” a particular way doesn’t support an answer, but merely presumes one. I am not here to argue the meaning of a word , not even if that word is “rationality.” The point of attaching sequences of letters to particular concepts is to let two people communicate —to help transport thoughts from one mind to another. You cannot change reality, or prove the thought, by manipulating which meanings go with which words. So if you understand what concept I am generally getting at with this word “rationality,” and with the sub-terms “epistemic rationality” and “instrumental rationality,” we have communicated : we have accomplished everything there 10\n\n--- Page 49 ---\nPredictably Wrong is to accomplish by talking about how to define “rationality.” What’s left to discuss is not what meaning to attach to the syllables “ra-tio-na-li-ty”; what’s left to discuss is what is a good way to think . If you say, “It’s (epistemically) rational for me to believe X , but the truth is Y , ” then you are probably using the word “rational” to mean something other than what I have in mind. (E.g., “rationality” should be consistent under reflec- tion —“rationally” looking at the evidence, and “rationally” considering how your mind processes the evidence, shouldn’t lead to two different conclusions.) Similarly, if you find yourself saying, “The (instrumentally) rational thing for me to do is X , but the right thing for me to do is Y , ” then you are almost cer- tainly using some other meaning for the word “rational” or the word “right.” I use the term “rationality” normatively , to pick out desirable patterns of thought. In this case—or in any other case where people disagree about word meanings—you should substitute more specific language in place of “ratio- nal”: “The self-benefiting thing to do is to run away, but I hope I would at least try to drag the child off the railroad tracks,” or “Causal decision theory as usually formulated says you should two-box on Newcomb’s Problem , but I’d rather have a million dollars.” In fact, I recommend reading back through this essay, replacing every instance of “rational” with “foozal,” and seeing if that changes the connotations of what I’m saying any. If so, I say: strive not for rationality, but for foozality. The word “rational” has potential pitfalls, but there are plenty of non -borderline cases where “rational” works fine to communicate what I’m getting at. Likewise “irrational.” In these cases I’m not afraid to use it. Yet one should be careful not to overuse that word. One receives no points merely for pronouncing it loudly. If you speak overmuch of the Way, you will not attain it. * 1. [ Editor’s Note: For a good introduction to Newcomb’s Problem, see Holt . 2 More generally, you can find definitions and explanations for many of the terms in this book at the website wiki.lesswrong.com/wiki/RAZ_Glossary.] 2. Jim Holt, “Thinking Inside the Boxes,” Slate (2002), http://www.slate.com/articles/arts/egghead/ 2002/02/thinkinginside%5C_the%5C_boxes.single.html. 11\n\n--- Page 50 ---\n2 Feeling Rational A popular belief about “rationality” is that rationality opposes all emotion—that all our sadness and all our joy are automatically anti-logical by virtue of being feelings. Yet strangely enough, I can’t find any theorem of probability theory which proves that I should appear ice-cold and expressionless. So is rationality orthogonal to feeling? No; our emotions arise from our models of reality. If I believe that my dead brother has been discovered alive, I will be happy; if I wake up and realize it was a dream, I will be sad. P. C. Hodgell said: “That which can be destroyed by the truth should be.” My dreaming self ’s happiness was opposed by truth. My sadness on waking is rational; there is no truth which destroys it. Rationality begins by asking how-the-world-is, but spreads virally to any other thought which depends on how we think the world is. Your beliefs about “how-the-world-is” can concern anything you think is out there in reality, anything that either does or does not exist, any member of the class “things that can make other things happen.” If you believe that there is a goblin in your closet that ties your shoes’ laces together, then this is a belief about how-the-world-is. Your shoes are real—you can pick them up. If there’s 12","elapsed_time_ms":0,"options":{"maxPages":50,"maxCharacters":100000}}